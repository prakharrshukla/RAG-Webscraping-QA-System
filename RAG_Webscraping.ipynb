{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5290d6b",
   "metadata": {},
   "source": [
    "# RAG Webscraping Q&A System\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This notebook demonstrates building a **Retrieval-Augmented Generation (RAG)** system that:\n",
    "- **Scrapes content** from any website\n",
    "- **Creates embeddings** from the scraped text\n",
    "- **Stores vectors** in a database for fast retrieval\n",
    "- **Uses a local LLM** (Ollama) to answer questions based on the scraped content\n",
    "- **Provides a web interface** (Streamlit) for user interaction\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Hardware Specifications & Technology Choices\n",
    "\n",
    "**System Configuration:**\n",
    "- **GPU**: RTX 4070 (8GB VRAM) - Perfect for running smaller LLMs locally\n",
    "- **CPU**: Intel i7-13620H - Handles embedding generation efficiently\n",
    "- **RAM**: 32GB - Allows smooth operation of multiple components\n",
    "\n",
    "**Technology Choices:**\n",
    "\n",
    "### üß† LLM: Ollama (llama3.2:1b)\n",
    "- **Reasoning**: With 8GB VRAM, larger models like 7B+ would be too slow\n",
    "- **Benefits**: 1B parameter model runs fast, uses ~2GB VRAM, good quality answers\n",
    "\n",
    "### üåê Frontend: Streamlit\n",
    "- **Reasoning**: Initially tried Voila but had JSON corruption issues\n",
    "- **Benefits**: Fast development, great for ML/AI projects, easy deployment\n",
    "\n",
    "### üìä Vector Database: ChromaDB\n",
    "- **Reasoning**: Lightweight, perfect for local development\n",
    "- **Benefits**: No server setup required, persistent storage, good performance\n",
    "\n",
    "### üî¢ Embeddings: SentenceTransformer (all-MiniLM-L6-v2)\n",
    "- **Reasoning**: Balanced between speed and quality\n",
    "- **Benefits**: Fast inference on CPU, good semantic understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbb7d0",
   "metadata": {},
   "source": [
    "## üìã Step 1: Environment Setup & Dependencies\n",
    "\n",
    "Install all required packages for our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'requests',           # For web scraping\n",
    "    'beautifulsoup4',     # For HTML parsing\n",
    "    'sentence-transformers', # For embeddings\n",
    "    'chromadb',           # Vector database\n",
    "    'langchain',          # RAG framework\n",
    "    'langchain-community', # Community integrations\n",
    "    'streamlit',          # Web interface\n",
    "    'ollama'              # Local LLM\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f\"‚úÖ {package} installed\")\n",
    "    except:\n",
    "        print(f\"‚ùå {package} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d807f5",
   "metadata": {},
   "source": [
    "## üìö Step 2: Import Libraries\n",
    "\n",
    "Import all necessary libraries for our RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ML & Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# LangChain RAG\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfe1cd",
   "metadata": {},
   "source": [
    "## üåê Step 3: Web Scraping Function\n",
    "\n",
    "Create a robust web scraper with enhanced headers to avoid bot detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url: str, max_length: int = 10000) -> str:\n",
    "    \"\"\"\n",
    "    Scrape content from website with bot detection avoidance.\n",
    "    \"\"\"\n",
    "    # Enhanced headers for Reuters, news sites, etc.\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Cache-Control': 'max-age=0'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"üåê Scraping: {url}\")\n",
    "        time.sleep(1)  # Be respectful\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        print(f\"‚úÖ Success (Status: {response.status_code})\")\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove scripts and styles\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Extract text from paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "        \n",
    "        if not content:\n",
    "            content = soup.get_text()\n",
    "        \n",
    "        # Clean and limit content\n",
    "        content = ' '.join(content.split())\n",
    "        if len(content) > max_length:\n",
    "            content = content[:max_length] + \"...\"\n",
    "        \n",
    "        print(f\"üìù Extracted {len(content)} characters\")\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return f\"Error scraping {url}: {str(e)}\"\n",
    "\n",
    "# Test scraping\n",
    "test_content = scrape_website(\"https://www.bbc.com/news/technology\", 1000)\n",
    "print(f\"\\nüìã Sample: {test_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afcab0",
   "metadata": {},
   "source": [
    "## üî¢ Step 4: Text Processing & Embeddings\n",
    "\n",
    "Process scraped text into chunks and create embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_for_embeddings(text: str, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Split text into chunks for better LLM processing.\n",
    "    \"\"\"\n",
    "    print(f\"üìù Processing {len(text)} characters\")\n",
    "    \n",
    "    # Smart text splitting\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"‚úÇÔ∏è Split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create Document objects\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"chunk_id\": i, \"total_chunks\": len(chunks)}\n",
    "        )\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test processing\n",
    "if test_content and \"Error\" not in test_content:\n",
    "    docs = process_text_for_embeddings(test_content)\n",
    "    print(f\"\\nüìÑ First chunk: {docs[0].page_content[:200]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping - no valid content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df718a0",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 5: Vector Database Setup\n",
    "\n",
    "Create ChromaDB vector store for fast similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_database(documents: List[Document], collection_name: str = None):\n",
    "    \"\"\"\n",
    "    Create vector database from documents.\n",
    "    \"\"\"\n",
    "    if not collection_name:\n",
    "        collection_name = f\"rag_collection_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    print(f\"üóÑÔ∏è Setting up vector DB: {collection_name}\")\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='all-MiniLM-L6-v2',\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Vector DB created with {len(documents)} documents\")\n",
    "    return vectorstore\n",
    "\n",
    "# Test vector database\n",
    "if 'docs' in locals() and docs:\n",
    "    vectorstore = setup_vector_database(docs, \"test_collection\")\n",
    "    \n",
    "    # Test similarity search\n",
    "    results = vectorstore.similarity_search(\"technology news\", k=2)\n",
    "    print(f\"\\nüîç Search results: {len(results)} documents found\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"Result {i+1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d0a658",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6: Ollama LLM Setup\n",
    "\n",
    "Configure local Ollama LLM optimized for RTX 4070:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16319790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ollama_llm(model_name: str = \"llama3.2:1b\"):\n",
    "    \"\"\"\n",
    "    Initialize Ollama LLM with optimized settings.\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Setting up Ollama: {model_name}\")\n",
    "    \n",
    "    llm = Ollama(\n",
    "        model=model_name,\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        temperature=0.1,    # Focused answers\n",
    "        top_p=0.9,         # Nucleus sampling\n",
    "        num_predict=512,   # Response length limit\n",
    "        repeat_penalty=1.1 # Reduce repetition\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LLM ready (optimized for RTX 4070)\")\n",
    "    return llm\n",
    "\n",
    "# Test Ollama\n",
    "try:\n",
    "    llm = setup_ollama_llm()\n",
    "    test_response = llm(\"What is AI?\")\n",
    "    print(f\"\\nüß™ Test response: {test_response[:150]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Ollama error: {e}\")\n",
    "    print(\"üí° Make sure Ollama is running:\")\n",
    "    print(\"   - ollama serve\")\n",
    "    print(\"   - ollama pull llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4324794",
   "metadata": {},
   "source": [
    "## üîó Step 7: Complete RAG Pipeline\n",
    "\n",
    "Combine all components into end-to-end system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4de272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    Create complete RAG chain.\n",
    "    \"\"\"\n",
    "    print(\"üîó Creating RAG chain...\")\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}  # Top 4 similar chunks\n",
    "    )\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG chain ready\")\n",
    "    return qa_chain\n",
    "\n",
    "def webscrape_rag_qa(url: str, question: str, fallback_urls: List[str] = None):\n",
    "    \"\"\"\n",
    "    Complete pipeline: scrape ‚Üí embed ‚Üí answer.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ RAG Pipeline: {url}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    \n",
    "    # Scrape content\n",
    "    content = scrape_website(url)\n",
    "    \n",
    "    # Try fallbacks if needed\n",
    "    if \"Error\" in content and fallback_urls:\n",
    "        for fallback in fallback_urls:\n",
    "            content = scrape_website(fallback)\n",
    "            if \"Error\" not in content:\n",
    "                url = fallback\n",
    "                break\n",
    "    \n",
    "    if \"Error\" in content or len(content.strip()) < 100:\n",
    "        return {\n",
    "            \"answer\": \"‚ùå Unable to scrape content\",\n",
    "            \"source_url\": url,\n",
    "            \"error\": content\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Process text\n",
    "        documents = process_text_for_embeddings(content)\n",
    "        \n",
    "        # Create vector DB\n",
    "        vectorstore = setup_vector_database(documents)\n",
    "        \n",
    "        # Setup LLM\n",
    "        llm = setup_ollama_llm()\n",
    "        \n",
    "        # Create RAG chain\n",
    "        qa_chain = create_rag_chain(vectorstore, llm)\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"ü§î Generating answer...\")\n",
    "        result = qa_chain({\"query\": question})\n",
    "        \n",
    "        print(\"‚úÖ Answer ready!\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"source_url\": url,\n",
    "            \"source_documents\": result[\"source_documents\"],\n",
    "            \"num_chunks\": len(documents)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"‚ùå Pipeline error: {str(e)}\",\n",
    "            \"source_url\": url,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"üéØ RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e5946",
   "metadata": {},
   "source": [
    "## üß™ Step 8: Test the Complete System\n",
    "\n",
    "Try our RAG system with a real example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete RAG system\n",
    "test_url = \"https://www.bbc.com/news/technology\"\n",
    "test_question = \"What are the main technology stories mentioned?\"\n",
    "\n",
    "fallback_urls = [\n",
    "    \"https://techcrunch.com\",\n",
    "    \"https://www.theverge.com\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG system...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result = webscrape_rag_qa(\n",
    "    url=test_url,\n",
    "    question=test_question,\n",
    "    fallback_urls=fallback_urls\n",
    ")\n",
    "\n",
    "print(\"\\nüìã RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üåê Source: {result.get('source_url', 'N/A')}\")\n",
    "print(f\"üìÑ Chunks: {result.get('num_chunks', 'N/A')}\")\n",
    "print(\"\\nüí¨ ANSWER:\")\n",
    "print(\"-\"*30)\n",
    "print(result['answer'])\n",
    "\n",
    "if 'source_documents' in result:\n",
    "    print(\"\\nüìö SOURCE CHUNKS:\")\n",
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        print(f\"\\nChunk {i+1}: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffc2bd",
   "metadata": {},
   "source": [
    "## üåü Step 9: Streamlit Web Interface\n",
    "\n",
    "The complete production application is available in `streamlit_rag_app.py`:\n",
    "\n",
    "### üöÄ To Run the Web Interface:\n",
    "1. **Start Ollama**: `ollama serve`\n",
    "2. **Install model**: `ollama pull llama3.2:1b`\n",
    "3. **Run Streamlit**: `streamlit run streamlit_rag_app.py`\n",
    "4. **Open browser**: http://localhost:8501\n",
    "\n",
    "### ‚úÖ Features:\n",
    "- User-friendly web interface\n",
    "- Real-time processing with progress bars\n",
    "- Website compatibility guidance\n",
    "- Error handling with helpful messages\n",
    "- Source document display for transparency\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Project Complete!\n",
    "\n",
    "### üìã What We Built:\n",
    "1. **Web Scraping** - Robust with bot detection avoidance\n",
    "2. **Text Processing** - Smart chunking with context preservation\n",
    "3. **Vector Database** - Fast similarity search with ChromaDB\n",
    "4. **Local LLM** - Ollama optimized for RTX 4070\n",
    "5. **RAG Pipeline** - Complete retrieval-augmented generation\n",
    "6. **Web Interface** - Production-ready Streamlit app\n",
    "\n",
    "### üåê GitHub Repository:\n",
    "**https://github.com/prakharrshukla/RAG-Webscraping-QA-System**\n",
    "\n",
    "### üí° Key Benefits:\n",
    "- **Hardware Optimized** for RTX 4070 (8GB VRAM)\n",
    "- **No API Costs** - Everything runs locally\n",
    "- **Privacy Focused** - No data sent externally\n",
    "- **Educational** - Complete step-by-step guide\n",
    "- **Production Ready** - Professional web interface\n",
    "\n",
    "**Happy RAG building! üéØ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
